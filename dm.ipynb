{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous presentation, I shared an interesting paper about a technique named dataset distillation, in which we may generate synthetic data to form a smaller dataset from the original large dataset. In this assignment, I will show the simple instructions to generate the synthetic dataset by using the distribution matching method. The main idea of the distribution matching is to calculate the empirical maximum mean discrepancy (MMD) between the original dataset $T$ and the synthetic dataset $S$:\n",
    "\n",
    "$$E_{\\theta \\sim P_\\theta}|| \\frac{1}{|T|} \\sum_{i=1}^{|T|} \\phi_\\theta (x_i) - \\frac{1}{|S|} \\sum_{i=1}^{|S|} \\phi_\\theta (s_i)||^2$$\n",
    "\n",
    "where we first sample the network parameter from the parameter space $P_\\theta$ and then construct an embedding function $\\phi_\\theta (s_i)$ for mapping the data from the distribution into a measure space. Therefore, we can calculate the distance between the two distributions of $T$ and $S$, and compute the gradient to minimize this distance. This paper adopted data augmentation while calculating the MMD; therefore, the final MMD will be:\n",
    "\n",
    "\n",
    "$$E_{\\theta \\sim P_\\theta}|| \\frac{1}{|T|} \\sum_{i=1}^{|T|} \\phi_\\theta (A(x_i)) - \\frac{1}{|S|} \\sum_{i=1}^{|S|} \\phi_\\theta (A(s_i))||^2$$\n",
    "\n",
    "where $A(*)$ is the data augmentation function.\n",
    "\n",
    "In this assignment, I will adopt ConvNet as the embedding function to generate the synthetic dataset for CIFAR10, and then evaluate this synthetic dataset by implementing the image classification task also with ResNet.\n",
    "\n",
    "## Reference \n",
    "B. Zhao and H. Bilen, \"Dataset Condensation with Distribution Matching,\" 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), Waikoloa, HI, USA, 2023, pp. 6503-6512, doi: 10.1109/WACV56688.2023.00645.\n",
    "\n",
    "https://github.com/VICO-UoE/DatasetCondensation/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import libraries and cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# iterations for generating images\n",
    "Iterations = 20000\n",
    "# image per class\n",
    "ipc = 10\n",
    "# learning rate for synthetic image\n",
    "lr_img = 1.0\n",
    "# batch size for real images\n",
    "batch_real = 256\n",
    "# dataset name\n",
    "dataset_name = 'cifar10'\n",
    "# save path\n",
    "save_path = './results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "channel = 3\n",
    "im_size = (32, 32)\n",
    "num_classes = 10\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "dst_train = datasets.CIFAR10('./data', train=True, download=True, transform=transform) # no augmentation\n",
    "dst_test = datasets.CIFAR10('./data', train=False, download=True, transform=transform)\n",
    "class_names = dst_train.classes\n",
    "testloader = torch.utils.data.DataLoader(dst_test, batch_size=256, shuffle=False, num_workers=0)\n",
    "lr_net = 0.01\n",
    "# get random n images from class c\n",
    "images_all = []\n",
    "labels_all = []\n",
    "indices_class = [[] for c in range(num_classes)]\n",
    "\n",
    "images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "for i, lab in enumerate(labels_all):\n",
    "    indices_class[lab].append(i)\n",
    "images_all = torch.cat(images_all, dim=0).to(device)\n",
    "labels_all = torch.tensor(labels_all, dtype=torch.long, device=device)\n",
    "def get_images(c, n): \n",
    "    idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "    return images_all[idx_shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Initalize the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_698706/2959516929.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  label_syn = torch.tensor([np.ones(ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n"
     ]
    }
   ],
   "source": [
    "# total number of images: number of classes * images per class\n",
    "# initialize the synthetic images with real data\n",
    "image_syn = torch.randn(size=(num_classes * ipc, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=device)\n",
    "for c in range(num_classes):\n",
    "    image_syn.data[c*ipc:(c+1)*ipc] = get_images(c, ipc).detach().data\n",
    "label_syn = torch.tensor([np.ones(ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Define the model\n",
    "Here, the embedding function and the model for evaluation are both using ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Swish activation '''\n",
    "class Swish(nn.Module): # Swish(x) = x∗σ(x)\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input * torch.sigmoid(input)\n",
    "\n",
    "''' ConvNet '''\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, channel, num_classes, net_width, net_depth, net_act, net_norm, net_pooling, im_size = (32,32)):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.features, shape_feat = self._make_layers(channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size)\n",
    "        num_feat = shape_feat[0]*shape_feat[1]*shape_feat[2]\n",
    "        self.classifier = nn.Linear(num_feat, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def embed(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out\n",
    "\n",
    "    def _get_activation(self, net_act):\n",
    "        if net_act == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        elif net_act == 'relu':\n",
    "            return nn.ReLU(inplace=True)\n",
    "        elif net_act == 'leakyrelu':\n",
    "            return nn.LeakyReLU(negative_slope=0.01)\n",
    "        elif net_act == 'swish':\n",
    "            return Swish()\n",
    "        else:\n",
    "            exit('unknown activation function: %s'%net_act)\n",
    "\n",
    "    def _get_pooling(self, net_pooling):\n",
    "        if net_pooling == 'maxpooling':\n",
    "            return nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif net_pooling == 'avgpooling':\n",
    "            return nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        elif net_pooling == 'none':\n",
    "            return None\n",
    "        else:\n",
    "            exit('unknown net_pooling: %s'%net_pooling)\n",
    "\n",
    "    def _get_normlayer(self, net_norm, shape_feat):\n",
    "        # shape_feat = (c*h*w)\n",
    "        if net_norm == 'batchnorm':\n",
    "            return nn.BatchNorm2d(shape_feat[0], affine=True)\n",
    "        elif net_norm == 'layernorm':\n",
    "            return nn.LayerNorm(shape_feat, elementwise_affine=True)\n",
    "        elif net_norm == 'instancenorm':\n",
    "            return nn.GroupNorm(shape_feat[0], shape_feat[0], affine=True)\n",
    "        elif net_norm == 'groupnorm':\n",
    "            return nn.GroupNorm(4, shape_feat[0], affine=True)\n",
    "        elif net_norm == 'none':\n",
    "            return None\n",
    "        else:\n",
    "            exit('unknown net_norm: %s'%net_norm)\n",
    "\n",
    "    def _make_layers(self, channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size):\n",
    "        layers = []\n",
    "        in_channels = channel\n",
    "        if im_size[0] == 28:\n",
    "            im_size = (32, 32)\n",
    "        shape_feat = [in_channels, im_size[0], im_size[1]]\n",
    "        for d in range(net_depth):\n",
    "            layers += [nn.Conv2d(in_channels, net_width, kernel_size=3, padding=3 if channel == 1 and d == 0 else 1)]\n",
    "            shape_feat[0] = net_width\n",
    "            if net_norm != 'none':\n",
    "                layers += [self._get_normlayer(net_norm, shape_feat)]\n",
    "            layers += [self._get_activation(net_act)]\n",
    "            in_channels = net_width\n",
    "            if net_pooling != 'none':\n",
    "                layers += [self._get_pooling(net_pooling)]\n",
    "                shape_feat[1] //= 2\n",
    "                shape_feat[2] //= 2\n",
    "\n",
    "        return nn.Sequential(*layers), shape_feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Define the data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters for data augmentation\n",
    "class ParamDiffAug():\n",
    "    def __init__(self):\n",
    "        self.aug_mode = 'S' #'multiple or single'\n",
    "        self.prob_flip = 0.5\n",
    "        self.ratio_scale = 1.2\n",
    "        self.ratio_rotate = 15.0\n",
    "        self.ratio_crop_pad = 0.125\n",
    "        self.ratio_cutout = 0.5 # the size would be 0.5x0.5\n",
    "        self.brightness = 1.0\n",
    "        self.saturation = 2.0\n",
    "        self.contrast = 0.5\n",
    "\n",
    "# setup the random seed for augmentation\n",
    "def set_seed_DiffAug(param):\n",
    "    if param.latestseed == -1:\n",
    "        return\n",
    "    else:\n",
    "        torch.random.manual_seed(param.latestseed)\n",
    "        param.latestseed += 1\n",
    "\n",
    "# We implement the following differentiable augmentation strategies based on the code provided in https://github.com/mit-han-lab/data-efficient-gans.\n",
    "def rand_scale(x, param):\n",
    "    # x>1, max scale\n",
    "    # sx, sy: (0, +oo), 1: orignial size, 0.5: enlarge 2 times\n",
    "    ratio = param.ratio_scale\n",
    "    set_seed_DiffAug(param)\n",
    "    sx = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n",
    "    set_seed_DiffAug(param)\n",
    "    sy = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n",
    "    theta = [[[sx[i], 0,  0],\n",
    "            [0,  sy[i], 0],] for i in range(x.shape[0])]\n",
    "    theta = torch.tensor(theta, dtype=torch.float)\n",
    "    if param.Siamese: # Siamese augmentation:\n",
    "        theta[:] = theta[0]\n",
    "    grid = F.affine_grid(theta, x.shape).to(x.device)\n",
    "    x = F.grid_sample(x, grid)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_rotate(x, param): # [-180, 180], 90: anticlockwise 90 degree\n",
    "    ratio = param.ratio_rotate\n",
    "    set_seed_DiffAug(param)\n",
    "    theta = (torch.rand(x.shape[0]) - 0.5) * 2 * ratio / 180 * float(np.pi)\n",
    "    theta = [[[torch.cos(theta[i]), torch.sin(-theta[i]), 0],\n",
    "        [torch.sin(theta[i]), torch.cos(theta[i]),  0],]  for i in range(x.shape[0])]\n",
    "    theta = torch.tensor(theta, dtype=torch.float)\n",
    "    if param.Siamese: # Siamese augmentation:\n",
    "        theta[:] = theta[0]\n",
    "    grid = F.affine_grid(theta, x.shape).to(x.device)\n",
    "    x = F.grid_sample(x, grid)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_flip(x, param):\n",
    "    prob = param.prob_flip\n",
    "    set_seed_DiffAug(param)\n",
    "    randf = torch.rand(x.size(0), 1, 1, 1, device=x.device)\n",
    "    if param.Siamese: # Siamese augmentation:\n",
    "        randf[:] = randf[0]\n",
    "    return torch.where(randf < prob, x.flip(3), x)\n",
    "\n",
    "\n",
    "def rand_brightness(x, param):\n",
    "    ratio = param.brightness\n",
    "    set_seed_DiffAug(param)\n",
    "    randb = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n",
    "    if param.Siamese:  # Siamese augmentation:\n",
    "        randb[:] = randb[0]\n",
    "    x = x + (randb - 0.5)*ratio\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x, param):\n",
    "    ratio = param.saturation\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    set_seed_DiffAug(param)\n",
    "    rands = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n",
    "    if param.Siamese:  # Siamese augmentation:\n",
    "        rands[:] = rands[0]\n",
    "    x = (x - x_mean) * (rands * ratio) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x, param):\n",
    "    ratio = param.contrast\n",
    "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
    "    set_seed_DiffAug(param)\n",
    "    randc = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n",
    "    if param.Siamese:  # Siamese augmentation:\n",
    "        randc[:] = randc[0]\n",
    "    x = (x - x_mean) * (randc + ratio) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_crop(x, param):\n",
    "    # The image is padded on its surrounding and then cropped.\n",
    "    ratio = param.ratio_crop_pad\n",
    "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    set_seed_DiffAug(param)\n",
    "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    set_seed_DiffAug(param)\n",
    "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    if param.Siamese:  # Siamese augmentation:\n",
    "        translation_x[:] = translation_x[0]\n",
    "        translation_y[:] = translation_y[0]\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
    "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
    "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, param):\n",
    "    ratio = param.ratio_cutout\n",
    "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    set_seed_DiffAug(param)\n",
    "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    set_seed_DiffAug(param)\n",
    "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    if param.Siamese:  # Siamese augmentation:\n",
    "        offset_x[:] = offset_x[0]\n",
    "        offset_y[:] = offset_y[0]\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
    "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
    "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
    "    mask[grid_batch, grid_x, grid_y] = 0\n",
    "    x = x * mask.unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'crop': [rand_crop],\n",
    "    'cutout': [rand_cutout],\n",
    "    'flip': [rand_flip],\n",
    "    'scale': [rand_scale],\n",
    "    'rotate': [rand_rotate],\n",
    "}\n",
    "def DiffAugment(x, strategy='', seed = -1, param = None):\n",
    "    if strategy == 'None' or strategy == 'none' or strategy == '':\n",
    "        return x\n",
    "\n",
    "    if seed == -1:\n",
    "        param.Siamese = False\n",
    "    else:\n",
    "        param.Siamese = True\n",
    "\n",
    "    param.latestseed = seed\n",
    "\n",
    "    if strategy:\n",
    "        if param.aug_mode == 'M': # original\n",
    "            for p in strategy.split('_'):\n",
    "                for f in AUGMENT_FNS[p]:\n",
    "                    x = f(x, param)\n",
    "        elif param.aug_mode == 'S':\n",
    "            pbties = strategy.split('_')\n",
    "            set_seed_DiffAug(param)\n",
    "            p = pbties[torch.randint(0, len(pbties), size=(1,)).item()]\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x, param)\n",
    "        else:\n",
    "            exit('unknown augmentation mode: %s'%param.aug_mode)\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "# data augmentation params\n",
    "dsa_param = ParamDiffAug()\n",
    "dsa_strategy = 'color_crop_cutout_flip_scale_rotate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Get the embedding function from the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(channel, num_classes, im_size=(32, 32)):\n",
    "    # set the different random seed for each run, so it will have different initial weights\n",
    "    net_width, net_depth, net_act, net_norm, net_pooling = 128, 3, 'relu', 'instancenorm', 'avgpooling'\n",
    "    torch.random.manual_seed(int(time.time() * 1000) % 100000)\n",
    "    net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    if device == 'cuda':\n",
    "        gpu_num = torch.cuda.device_count()\n",
    "        if gpu_num>1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net = net.to(device)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. The training iteration for generating the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guojun/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:4377: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/guojun/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:4316: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/guojun/anaconda3/lib/python3.11/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1704987288773/work/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 00000, loss = 24.1172\n",
      "iter = 00100, loss = 7.5307\n",
      "iter = 00200, loss = 6.6096\n",
      "iter = 00300, loss = 5.9546\n",
      "iter = 00400, loss = 5.9315\n",
      "iter = 00500, loss = 5.7106\n",
      "iter = 00600, loss = 5.5293\n",
      "iter = 00700, loss = 5.0596\n",
      "iter = 00800, loss = 5.1546\n",
      "iter = 00900, loss = 5.0365\n",
      "iter = 01000, loss = 4.7591\n",
      "iter = 01100, loss = 5.0294\n",
      "iter = 01200, loss = 4.8128\n",
      "iter = 01300, loss = 4.7329\n",
      "iter = 01400, loss = 4.6285\n",
      "iter = 01500, loss = 5.2236\n",
      "iter = 01600, loss = 4.6348\n",
      "iter = 01700, loss = 4.6310\n",
      "iter = 01800, loss = 4.4247\n",
      "iter = 01900, loss = 4.5451\n",
      "iter = 02000, loss = 4.6869\n",
      "iter = 02100, loss = 4.2667\n",
      "iter = 02200, loss = 4.2211\n",
      "iter = 02300, loss = 4.6221\n",
      "iter = 02400, loss = 4.3642\n",
      "iter = 02500, loss = 4.3483\n",
      "iter = 02600, loss = 4.2857\n",
      "iter = 02700, loss = 4.2776\n",
      "iter = 02800, loss = 4.1695\n",
      "iter = 02900, loss = 4.3999\n",
      "iter = 03000, loss = 4.6242\n",
      "iter = 03100, loss = 4.2816\n",
      "iter = 03200, loss = 4.1790\n",
      "iter = 03300, loss = 4.0455\n",
      "iter = 03400, loss = 4.1376\n",
      "iter = 03500, loss = 4.2715\n",
      "iter = 03600, loss = 4.2307\n",
      "iter = 03700, loss = 4.1268\n",
      "iter = 03800, loss = 4.2796\n",
      "iter = 03900, loss = 4.1532\n",
      "iter = 04000, loss = 4.1591\n",
      "iter = 04100, loss = 4.1940\n",
      "iter = 04200, loss = 4.1230\n",
      "iter = 04300, loss = 3.9303\n",
      "iter = 04400, loss = 4.1085\n",
      "iter = 04500, loss = 4.1446\n",
      "iter = 04600, loss = 4.0889\n",
      "iter = 04700, loss = 4.1217\n",
      "iter = 04800, loss = 4.1502\n",
      "iter = 04900, loss = 4.2115\n",
      "iter = 05000, loss = 4.0832\n",
      "iter = 05100, loss = 4.1506\n",
      "iter = 05200, loss = 3.9623\n",
      "iter = 05300, loss = 4.1240\n",
      "iter = 05400, loss = 4.1007\n",
      "iter = 05500, loss = 4.2093\n",
      "iter = 05600, loss = 4.3539\n",
      "iter = 05700, loss = 4.1111\n",
      "iter = 05800, loss = 4.1658\n",
      "iter = 05900, loss = 3.9136\n",
      "iter = 06000, loss = 4.0893\n",
      "iter = 06100, loss = 3.9649\n",
      "iter = 06200, loss = 4.1885\n",
      "iter = 06300, loss = 3.7748\n",
      "iter = 06400, loss = 4.2682\n",
      "iter = 06500, loss = 4.3709\n",
      "iter = 06600, loss = 3.8994\n",
      "iter = 06700, loss = 4.0699\n",
      "iter = 06800, loss = 3.8902\n",
      "iter = 06900, loss = 3.9260\n",
      "iter = 07000, loss = 4.2371\n",
      "iter = 07100, loss = 4.1237\n",
      "iter = 07200, loss = 4.0826\n",
      "iter = 07300, loss = 4.0729\n",
      "iter = 07400, loss = 4.2858\n",
      "iter = 07500, loss = 3.9296\n",
      "iter = 07600, loss = 4.0200\n",
      "iter = 07700, loss = 3.9745\n",
      "iter = 07800, loss = 4.1806\n",
      "iter = 07900, loss = 4.2083\n",
      "iter = 08000, loss = 3.9646\n",
      "iter = 08100, loss = 3.8501\n",
      "iter = 08200, loss = 4.1713\n",
      "iter = 08300, loss = 4.0833\n",
      "iter = 08400, loss = 4.1953\n",
      "iter = 08500, loss = 3.9777\n",
      "iter = 08600, loss = 4.0002\n",
      "iter = 08700, loss = 4.1050\n",
      "iter = 08800, loss = 3.8619\n",
      "iter = 08900, loss = 3.8074\n",
      "iter = 09000, loss = 3.9454\n",
      "iter = 09100, loss = 4.0169\n",
      "iter = 09200, loss = 4.0958\n",
      "iter = 09300, loss = 4.0564\n",
      "iter = 09400, loss = 4.0243\n",
      "iter = 09500, loss = 4.0428\n",
      "iter = 09600, loss = 3.9604\n",
      "iter = 09700, loss = 4.0073\n",
      "iter = 09800, loss = 4.0374\n",
      "iter = 09900, loss = 4.0084\n",
      "iter = 10000, loss = 3.8615\n",
      "iter = 10100, loss = 4.1913\n",
      "iter = 10200, loss = 3.8081\n",
      "iter = 10300, loss = 3.8658\n",
      "iter = 10400, loss = 3.9746\n",
      "iter = 10500, loss = 3.9108\n",
      "iter = 10600, loss = 3.7116\n",
      "iter = 10700, loss = 3.8159\n",
      "iter = 10800, loss = 3.7334\n",
      "iter = 10900, loss = 3.8704\n",
      "iter = 11000, loss = 3.8090\n",
      "iter = 11100, loss = 4.1264\n",
      "iter = 11200, loss = 3.5153\n",
      "iter = 11300, loss = 3.9761\n",
      "iter = 11400, loss = 4.1196\n",
      "iter = 11500, loss = 4.1209\n",
      "iter = 11600, loss = 3.9407\n",
      "iter = 11700, loss = 4.2058\n",
      "iter = 11800, loss = 3.9232\n",
      "iter = 11900, loss = 4.0555\n",
      "iter = 12000, loss = 3.9859\n",
      "iter = 12100, loss = 3.5914\n",
      "iter = 12200, loss = 3.8937\n",
      "iter = 12300, loss = 3.7858\n",
      "iter = 12400, loss = 3.6741\n",
      "iter = 12500, loss = 4.1452\n",
      "iter = 12600, loss = 3.9309\n",
      "iter = 12700, loss = 3.8045\n",
      "iter = 12800, loss = 3.7142\n",
      "iter = 12900, loss = 3.8958\n",
      "iter = 13000, loss = 4.2880\n",
      "iter = 13100, loss = 3.7749\n",
      "iter = 13200, loss = 4.0252\n",
      "iter = 13300, loss = 4.1108\n",
      "iter = 13400, loss = 3.8230\n",
      "iter = 13500, loss = 3.6775\n",
      "iter = 13600, loss = 4.1757\n",
      "iter = 13700, loss = 4.1494\n",
      "iter = 13800, loss = 3.8484\n",
      "iter = 13900, loss = 3.7425\n",
      "iter = 14000, loss = 3.8291\n",
      "iter = 14100, loss = 3.7068\n",
      "iter = 14200, loss = 3.9103\n",
      "iter = 14300, loss = 3.6491\n",
      "iter = 14400, loss = 4.0661\n",
      "iter = 14500, loss = 3.8627\n",
      "iter = 14600, loss = 3.7999\n",
      "iter = 14700, loss = 3.6895\n",
      "iter = 14800, loss = 3.7307\n",
      "iter = 14900, loss = 4.0576\n",
      "iter = 15000, loss = 3.7480\n",
      "iter = 15100, loss = 3.9138\n",
      "iter = 15200, loss = 4.0133\n",
      "iter = 15300, loss = 3.6200\n",
      "iter = 15400, loss = 4.0249\n",
      "iter = 15500, loss = 3.7121\n",
      "iter = 15600, loss = 4.0799\n",
      "iter = 15700, loss = 3.8548\n",
      "iter = 15800, loss = 3.9032\n",
      "iter = 15900, loss = 3.7520\n",
      "iter = 16000, loss = 3.9430\n",
      "iter = 16100, loss = 3.7133\n",
      "iter = 16200, loss = 3.8409\n",
      "iter = 16300, loss = 3.7247\n",
      "iter = 16400, loss = 4.0869\n",
      "iter = 16500, loss = 3.9159\n",
      "iter = 16600, loss = 3.7637\n",
      "iter = 16700, loss = 3.7092\n",
      "iter = 16800, loss = 4.0454\n",
      "iter = 16900, loss = 3.6326\n",
      "iter = 17000, loss = 4.0599\n",
      "iter = 17100, loss = 3.8989\n",
      "iter = 17200, loss = 3.5422\n",
      "iter = 17300, loss = 4.0242\n",
      "iter = 17400, loss = 3.8367\n",
      "iter = 17500, loss = 3.8219\n",
      "iter = 17600, loss = 3.8488\n",
      "iter = 17700, loss = 3.8892\n",
      "iter = 17800, loss = 4.1022\n",
      "iter = 17900, loss = 3.9022\n",
      "iter = 18000, loss = 3.5389\n",
      "iter = 18100, loss = 3.8187\n",
      "iter = 18200, loss = 3.7306\n",
      "iter = 18300, loss = 3.8852\n",
      "iter = 18400, loss = 3.7965\n",
      "iter = 18500, loss = 3.6956\n",
      "iter = 18600, loss = 3.8000\n",
      "iter = 18700, loss = 3.7463\n",
      "iter = 18800, loss = 3.7870\n",
      "iter = 18900, loss = 3.8163\n",
      "iter = 19000, loss = 3.7078\n",
      "iter = 19100, loss = 4.0310\n",
      "iter = 19200, loss = 3.7244\n",
      "iter = 19300, loss = 3.5789\n",
      "iter = 19400, loss = 3.7871\n",
      "iter = 19500, loss = 3.8881\n",
      "iter = 19600, loss = 3.8756\n",
      "iter = 19700, loss = 3.8623\n",
      "iter = 19800, loss = 3.8675\n",
      "iter = 19900, loss = 3.9905\n",
      "iter = 20000, loss = 3.8146\n"
     ]
    }
   ],
   "source": [
    "optimizer_img = torch.optim.SGD([image_syn, ], lr=lr_img, momentum=0.5) # optimizer_img for synthetic data\n",
    "optimizer_img.zero_grad()\n",
    "for it in range(Iterations + 1):\n",
    "    ''' Train synthetic data '''\n",
    "    net = get_network(channel, num_classes).to(device) # get a random model\n",
    "    net.train()\n",
    "    for param in list(net.parameters()):\n",
    "        param.requires_grad = False\n",
    "    embed = net.module.embed if torch.cuda.device_count() > 1 else net.embed # for GPU parallel\n",
    "\n",
    "    loss_avg = 0\n",
    "\n",
    "    ''' update synthetic data '''\n",
    "    loss = torch.tensor(0.0).to(device)\n",
    "    for c in range(num_classes):\n",
    "        img_real = get_images(c, batch_real)\n",
    "        img_syn = image_syn[c*ipc:(c+1)*ipc].reshape((ipc, channel, im_size[0], im_size[1]))\n",
    "\n",
    "        seed = int(time.time() * 1000) % 100000\n",
    "        img_real = DiffAugment(img_real, dsa_strategy, seed=seed, param=dsa_param)\n",
    "        img_syn = DiffAugment(img_syn, dsa_strategy, seed=seed, param=dsa_param)\n",
    "\n",
    "        output_real = embed(img_real).detach()\n",
    "        output_syn = embed(img_syn)\n",
    "\n",
    "        loss += torch.sum((torch.mean(output_real, dim=0) - torch.mean(output_syn, dim=0))**2)\n",
    "\n",
    "    optimizer_img.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_img.step()\n",
    "    loss_avg += loss.item()\n",
    "\n",
    "\n",
    "    loss_avg /= (num_classes)\n",
    "\n",
    "    if it % 100 == 0:\n",
    "        print('iter = %05d, loss = %.4f' % (it, loss_avg))\n",
    "\n",
    "        ''' visualize and save '''\n",
    "        save_name = os.path.join(save_path, 'vis_%s_%dipc_iter%d.png'%(dataset_name, ipc, it))\n",
    "        image_syn_vis = copy.deepcopy(image_syn.detach().cpu())\n",
    "        for ch in range(channel):\n",
    "            image_syn_vis[:, ch] = image_syn_vis[:, ch]  * std[ch] + mean[ch]\n",
    "        image_syn_vis[image_syn_vis<0] = 0.0\n",
    "        image_syn_vis[image_syn_vis>1] = 1.0\n",
    "        save_image(image_syn_vis, save_name, nrow=ipc) # Trying normalize = True/False may get better visual effects.\n",
    "\n",
    "    if it == Iterations: # only record the final results\n",
    "        data_save = []\n",
    "        data_save.append([copy.deepcopy(image_syn.detach().cpu()), copy.deepcopy(label_syn.detach().cpu())])\n",
    "        torch.save({'data': data_save}, os.path.join(save_path, 'res_%s_%dipc.pt'%(dataset_name, ipc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Visualization for the result\n",
    "The distilled dataset from CIFAR10:\n",
    "\n",
    "<img src=\"./vis_cifar10_10ipc_iter20000.png\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Evaluate the synthetic data by classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate: test loss = 2.8109, test acc = 0.4302\n"
     ]
    }
   ],
   "source": [
    "def epoch(mode, dataloader, net, optimizer, criterion, aug):\n",
    "    loss_avg, acc_avg, num_exp = 0, 0, 0\n",
    "    net = net.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    if mode == 'train':\n",
    "        net.train()\n",
    "    else:\n",
    "        net.eval()\n",
    "\n",
    "    for i_batch, datum in enumerate(dataloader):\n",
    "        img = datum[0].float().to(device)\n",
    "        if aug:\n",
    "            img = DiffAugment(img, dsa_strategy, param=dsa_param)\n",
    "        lab = datum[1].long().to(device)\n",
    "        n_b = lab.shape[0]\n",
    "\n",
    "        output = net(img)\n",
    "        loss = criterion(output, lab)\n",
    "        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n",
    "\n",
    "        loss_avg += loss.item()*n_b\n",
    "        acc_avg += acc\n",
    "        num_exp += n_b\n",
    "\n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    loss_avg /= num_exp\n",
    "    acc_avg /= num_exp\n",
    "\n",
    "    return loss_avg, acc_avg\n",
    "\n",
    "\n",
    "# parameters for network training\n",
    "lr = 0.1\n",
    "Epoch = 1000\n",
    "batch_train = 256\n",
    "net = get_network(channel, num_classes).to(device) # get a random model\n",
    "images_train = image_syn.to(device)\n",
    "labels_train = label_syn.to(device)\n",
    "lr_schedule = [Epoch//2+1]\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "dst_train = TensorDataset(images_train, labels_train)\n",
    "trainloader = torch.utils.data.DataLoader(dst_train, batch_size=batch_train, shuffle=True, num_workers=0)\n",
    "\n",
    "for ep in range(Epoch+1):\n",
    "    loss_train, acc_train = epoch('train', trainloader, net, optimizer, criterion,  aug = True)\n",
    "    if ep in lr_schedule:\n",
    "        lr *= 0.1\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "loss_test, acc_test = epoch('test', testloader, net, optimizer, criterion, aug = False)\n",
    "print('Evaluate: test loss = %.4f, test acc = %.4f' % (loss_test, acc_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
